name: export-sense-voice-to-qnn

on:
  push:
    branches:
      - export-sense-voice-qnn
  workflow_dispatch:

concurrency:
  group: export-sense-voice-to-qnn-${{ github.ref }}
  cancel-in-progress: true

jobs:
  export-sense-voice-to-qnn:
    if: github.repository_owner == 'k2-fsa' || github.repository_owner == 'csukuangfj'
    name: ${{ matrix.framework }} ${{ matrix.platform }} ${{ matrix.input_in_seconds }}
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-22.04]
        python-version: ["3.10"]
        # input_in_seconds: ["5", "8", "10", "13", "15", "18", "20", "23", "25", "28", "30"]
        input_in_seconds: ["5"]
        framework: ["FunASR", "WSYue-ASR"]

    steps:
      - uses: actions/checkout@v4

      - name: Setup Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}

      - name: Display NDK HOME
        shell: bash
        run: |
          echo "ANDROID_NDK_LATEST_HOME: ${ANDROID_NDK_LATEST_HOME}"
          ls -lh ${ANDROID_NDK_LATEST_HOME}

      - name: Create Python virtual environment
        shell: bash
        run: |
          python3 -m venv py310
          which python3
          source py310/bin/activate
          which python3

      - name: Show ndk-build help
        shell: bash
        run: |
          export PATH=${ANDROID_NDK_LATEST_HOME}:$PATH
          ndk-build --help

      - name: Download toolkit
        shell: bash
        run: |
          curl -SL -O https://huggingface.co/csukuangfj/qnn-toolkit/resolve/main/v2.32.6.250402.zip
          ls -lh v2.32.6.250402.zip

      - name: Unzip toolkit
        shell: bash
        run: |
          unzip v2.32.6.250402.zip

      - name: Install linux dependencies
        shell: bash
        run: |
          cd qairt/2.32.6.250402/bin
          source envsetup.sh

          yes | sudo ${QNN_SDK_ROOT}/bin/check-linux-dependency.sh || true

      - name: Install Python dependencies
        shell: bash
        run: |
          source py310/bin/activate

          cd qairt/2.32.6.250402/bin
          source envsetup.sh

          python3 "${QNN_SDK_ROOT}/bin/check-python-dependency"

          which python3

      - name: Install onnx dependencies
        shell: bash
        run: |
          source py310/bin/activate
          python3 -m pip install --upgrade \
            torch==2.0.0+cpu -f https://download.pytorch.org/whl/torch \
            kaldi_native_fbank \
            pip \
            "numpy<2" \
            onnx==1.17.0 \
            onnxruntime==1.17.1 \
            soundfile \
            librosa \
            onnxsim \
            sentencepiece \
            pyyaml

          which python3

      - name: Show qnn-onnx-converter help
        shell: bash
        run: |
          source py310/bin/activate

          pushd qairt/2.32.6.250402/bin
          source envsetup.sh
          popd

          qnn-onnx-converter --help

      - name: Show qnn-model-lib-generator help
        shell: bash
        run: |
          source py310/bin/activate

          pushd qairt/2.32.6.250402/bin
          source envsetup.sh
          popd

          qnn-model-lib-generator --help

      - name: Show qnn-net-run help
        shell: bash
        run: |
          source py310/bin/activate

          pushd qairt/2.32.6.250402/bin
          source envsetup.sh
          popd

          qnn-net-run --help

      - name: Run SenseVoice from FunAsr
        if: matrix.framework == 'FunASR'
        shell: bash
        run: |
          source py310/bin/activate

          pushd qairt/2.32.6.250402/bin
          source envsetup.sh
          popd

          export PATH=${ANDROID_NDK_LATEST_HOME}:$PATH
          export LDFLAGS="-Wl,-z,max-page-size=16384"

          cd scripts/sense-voice/qnn

          curl -SL -O https://hf-mirror.com/FunAudioLLM/SenseVoiceSmall/resolve/main/am.mvn
          curl -SL -O https://hf-mirror.com/FunAudioLLM/SenseVoiceSmall/resolve/main/model.pt
          curl -SL -O https://hf-mirror.com/FunAudioLLM/SenseVoiceSmall/resolve/main/chn_jpn_yue_eng_ko_spectok.bpe.model

          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17/resolve/main/test_wavs/en.wav
          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17/resolve/main/test_wavs/ja.wav
          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17/resolve/main/test_wavs/ko.wav
          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17/resolve/main/test_wavs/yue.wav
          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17/resolve/main/test_wavs/zh.wav

          rm -f README.md || true

          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17/resolve/main/README.md
          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17/resolve/main/LICENSE

          echo "export to onnx"
          t=${{ matrix.input_in_seconds }}

          echo "----$t---"
          python3 ./export-onnx.py --input-len-in-seconds $t --opset-version 9

          ls -lh *.onnx

          python3 ../../pyannote/segmentation/show-onnx.py --filename ./model-$t-seconds.onnx

          echo "test exported onnx models"

          echo "----------$t----------"
          python3 ./test_onnx.py --model model-$t-seconds.onnx --tokens ./tokens.txt --wave ./en.wav
          python3 ./test_onnx.py --model model-$t-seconds.onnx --tokens ./tokens.txt --wave ./ja.wav
          python3 ./test_onnx.py --model model-$t-seconds.onnx --tokens ./tokens.txt --wave ./ko.wav
          python3 ./test_onnx.py --model model-$t-seconds.onnx --tokens ./tokens.txt --wave ./yue.wav
          python3 ./test_onnx.py --model model-$t-seconds.onnx --tokens ./tokens.txt --wave ./zh.wav

          echo "export to qnn"
          echo "----------$t----------"
          num_frames=$(python3 -c "print(int($t*100 / 6 + 0.5))")

          echo "num_frames: $num_frames"

          if true; then
            qnn-onnx-converter \
              --input_network ./model-$t-seconds.onnx \
              --output_path ./model \
              --out_node logits \
              --input_dtype x float32 \
              --input_dtype prompt int32 \
              --input_layout x NTF \
              --input_dim x 1,$num_frames,560 \
              --input_dim prompt 4
          else
            qnn-onnx-converter \
              --input_network ./model-$t-seconds.onnx \
              --output_path ./model \
              --out_node logits \
              --input_dtype x float32 \
              --input_dtype prompt int32 \
              --input_layout x NTF \
              --float_bitwidth 16 \
              --use_per_row_quantization \
              --input_dim x 1,$num_frames,560 \
              --input_dim prompt 4
          fi


          ls -lh model*
          mv model model.cpp
          echo "---"
          ls -lh model*

          python3 "${QNN_SDK_ROOT}/bin/x86_64-linux-clang/qnn-model-lib-generator" \
            -c "model.cpp" \
            -b "model.bin" \
            -o model_libs > /dev/null 2>&1

          ls -lh model_libs/*/

          readelf -lW model_libs/*/lib*.so

          echo "test qnn model"

          if false; then
            for wav in en.wav ja.wav ko.wav yue.wav zh.wav; do
              ./generate_test_data.py --num-frames $num_frames --wav $wav
              echo "$PWD/features.raw $PWD/prompt.raw" > input_list.txt

              ${QNN_SDK_ROOT}/bin/x86_64-linux-clang/qnn-net-run \
                --log_level info \
                --model ./model_libs/x86_64-linux-clang/libmodel.so \
                --backend ${QNN_SDK_ROOT}/lib/x86_64-linux-clang/libQnnCpu.so \
                --input_list ./input_list.txt \
                --use_native_input_files=true \
                --output_dir ./output_qnn_x64

              mv output_qnn_x64/Result_0/logits.raw ./

              ./decode_logits.py
            done
          fi

          echo "collect results"

          for p in x86_64-linux-clang aarch64-android; do
            if [[ $p == x86_64-linux-clang ]]; then
              d=sherpa-onnx-qnn-$t-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-linux-x64
            elif [[ $p == aarch64-android ]]; then
              d=sherpa-onnx-qnn-$t-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-android-aarch64
            else
              echo "Unknown $p"
              exit -1
            fi

            mkdir -p $d
            mkdir -p $d/test_wavs

            cp -v README.md $d
            cp -v LICENSE $d
            cp -v model_libs/$p/lib*.so $d/
            cp -v tokens.txt $d
            cp -v *.wav $d/test_wavs

            echo "num_frames=$num_frames" > $d/info.txt
            echo "target=$p" >> $d/info.txt

            ls -lh $d
            tar cjfv $d.tar.bz2 $d
            ls -lh *.tar.bz2
            rm -rf $d
          done

          echo "----show---"
          ls -lh *.tar.bz2

          mv *.tar.bz2 ../../..

      - name: Run SenseVoice from WSYue-ASR
        if: matrix.framework == 'WSYue-ASR'
        shell: bash
        run: |
          source py310/bin/activate

          pushd qairt/2.32.6.250402/bin
          source envsetup.sh
          popd

          export PATH=${ANDROID_NDK_LATEST_HOME}:$PATH
          export LDFLAGS="-Wl,-z,max-page-size=16384"

          cd scripts/sense-voice/qnn

          curl -SL -O https://huggingface.co/ASLP-lab/WSYue-ASR/resolve/main/sensevoice_small_yue/model.pt

          curl -SL -O https://hf-mirror.com/FunAudioLLM/SenseVoiceSmall/resolve/main/am.mvn
          curl -SL -O https://hf-mirror.com/FunAudioLLM/SenseVoiceSmall/resolve/main/chn_jpn_yue_eng_ko_spectok.bpe.model

          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17/resolve/main/test_wavs/en.wav
          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17/resolve/main/test_wavs/yue.wav
          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17/resolve/main/test_wavs/zh.wav

          for i in $(seq 0 17); do
            curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-sense-voice-zh-en-ja-ko-yue-int8-2025-09-09/resolve/main/test_wavs/yue-$i.wav
          done

          rm -f README.md || true

          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-sense-voice-zh-en-ja-ko-yue-int8-2025-09-09/resolve/main/README.md

          echo "export to onnx"
          t=${{ matrix.input_in_seconds }}

          echo "----$t---"

          export model_author="ASLP-lab"
          export comment="ASLP-lab/WSYue-ASR"
          export url="https://huggingface.co/ASLP-lab/WSYue-ASR/tree/main/sensevoice_small_yue"

          python3 ./export-onnx.py --input-len-in-seconds $t --opset-version 9

          ls -lh *.onnx

          python3 ../../pyannote/segmentation/show-onnx.py --filename ./model-$t-seconds.onnx

          echo "test exported onnx models"

          echo "----------$t----------"
          python3 ./test_onnx.py --model model-$t-seconds.onnx --tokens ./tokens.txt --wave ./en.wav
          python3 ./test_onnx.py --model model-$t-seconds.onnx --tokens ./tokens.txt --wave ./yue.wav
          python3 ./test_onnx.py --model model-$t-seconds.onnx --tokens ./tokens.txt --wave ./zh.wav
          for i in $(seq 0 17); do
            echo "yue-$i.wav"
            python3 ./test_onnx.py --model model-$t-seconds.onnx --tokens ./tokens.txt --wave ./yue-$i.wav
          done

          echo "export to qnn"
          echo "----------$t----------"
          num_frames=$(python3 -c "print(int($t*100 / 6 + 0.5))")

          echo "num_frames: $num_frames"

          if true; then
            qnn-onnx-converter \
              --input_network ./model-$t-seconds.onnx \
              --output_path ./model \
              --out_node logits \
              --input_dtype x float32 \
              --input_dtype prompt int32 \
              --input_layout x NTF \
              --input_dim x 1,$num_frames,560 \
              --input_dim prompt 4
          else
            qnn-onnx-converter \
              --input_network ./model-$t-seconds.onnx \
              --output_path ./model \
              --out_node logits \
              --input_dtype x float32 \
              --input_dtype prompt int32 \
              --input_layout x NTF \
              --float_bitwidth 16 \
              --use_per_row_quantization \
              --input_dim x 1,$num_frames,560 \
              --input_dim prompt 4
          fi

          ls -lh model*
          mv model model.cpp
          echo "---"
          ls -lh model*

          python3 "${QNN_SDK_ROOT}/bin/x86_64-linux-clang/qnn-model-lib-generator" \
            -c "model.cpp" \
            -b "model.bin" \
            -o model_libs > /dev/null 2>&1

          ls -lh model_libs/*/

          if false; then
            for wav in en.wav yue.wav zh.wav; do
              ./generate_test_data.py --num-frames $num_frames --wav $wav

              echo "$PWD/features.raw $PWD/prompt.raw" > input_list.txt

              ${QNN_SDK_ROOT}/bin/x86_64-linux-clang/qnn-net-run \
                --log_level info \
                --model ./model_libs/x86_64-linux-clang/libmodel.so \
                --backend ${QNN_SDK_ROOT}/lib/x86_64-linux-clang/libQnnCpu.so \
                --input_list ./input_list.txt \
                --use_native_input_files=true \
                --output_dir ./output_qnn_x64

              mv output_qnn_x64/Result_0/logits.raw ./

              ./decode_logits.py
            done
          fi

          echo "collect results"
          for p in x86_64-linux-clang aarch64-android; do
            if [[ $p == x86_64-linux-clang ]]; then
              d=sherpa-onnx-qnn-$t-seconds-sense-voice-zh-en-ja-ko-yue-2025-09-09-linux-x64
            elif [[ $p == aarch64-android ]]; then
              d=sherpa-onnx-qnn-$t-seconds-sense-voice-zh-en-ja-ko-yue-2025-09-09-android-aarch64
            else
              echo "Unknown $p"
              exit -1
            fi

            mkdir -p $d
            mkdir -p $d/test_wavs

            cp -v README.md $d
            cp -v model_libs/$p/lib*.so $d/
            cp -v tokens.txt $d
            cp -v *.wav $d/test_wavs

            echo "num_frames=$num_frames" > $d/info.txt
            echo "target=$p" >> $d/info.txt

            ls -lh $d
            tar cjfv $d.tar.bz2 $d
            ls -lh *.tar.bz2
            rm -rf $d
          done

          echo "----show---"
          ls -lh *.tar.bz2

          mv *.tar.bz2 ../../..

      - name: Release
        if: github.repository_owner == 'csukuangfj'
        uses: svenstaro/upload-release-action@v2
        with:
          file_glob: true
          file: ./*.tar.bz2
          overwrite: true
          repo_name: k2-fsa/sherpa-onnx
          repo_token: ${{ secrets.UPLOAD_GH_SHERPA_ONNX_TOKEN }}
          tag: asr-models

      - name: Release
        if: github.repository_owner == 'k2-fsa'
        uses: svenstaro/upload-release-action@v2
        with:
          file_glob: true
          file: ./*.tar.bz2
          overwrite: true
          tag: asr-models
